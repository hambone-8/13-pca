{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "Authors: _Kiefer Katovich (SF), Adam Jones, Steven Longstreet(DC)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "After this lesson, you will be able to:\n",
    "\n",
    "- Describe what principal component analysis (PCA) does and what it’s used for in data science.\n",
    "- Understand key terms, including _principal components, eigenvalues, and eigenvectors_.\n",
    "- Practice computing PCA manually (and automatically, with scikit-learn).\n",
    "- Represent PCA graphically and interpret results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Motivation](#motivation)\n",
    "- [What is PCA?](#what)\n",
    "    - [Eigenvalues and Eigenvectors](#eigenpairs)\n",
    "    - [Principal Components](#pcs)\n",
    "- [Manual PCA Code Along](#manual)\n",
    "    - [1) Basic EDA](#basic-eda)\n",
    "    - [2) Subset and Normalize](#subset)\n",
    "    - [3) Find the Correlation Matrix](#corr)\n",
    "    - [4) Eigenvalues and Eigenvectors](#eigen)\n",
    "    - [5) Explained Variance](#var)\n",
    "    - [6) Projection Matrix W](#projection)\n",
    "    - [7) Transformed Matrix Z](#transformed)\n",
    "    - [8–12) Plot and Interpret Principal Components](#plot-components)\n",
    "- [PCA in scikit-learn](#auto)\n",
    "- [Example: Eigenfaces](#example)\n",
    "- [Further Reading](#further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:49.384566Z",
     "start_time": "2022-02-08T10:50:43.073720Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # registers the 3D projection, but is otherwise unused\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.datasets import load_diabetes\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "PCA isn't exactly full machine learning algorithm, but instead an unsupervised learning algorithm. It is often used to **preprocess** data before it goes into a supervised learning method. Traditionally it is used to solve problems involving too many features and multicolinearity. \n",
    "\n",
    "## Let's dig into how it works \n",
    "Suppose you have $p$ feature columns. The **first principal component** is a linear combination of all $p$ columns that accounts for the **maximum variance** among them.  That is,\n",
    "\n",
    "$$ z_1 = c_{11}x_1 + c_{12}x_2 + c_{13}x_3 + ... c_{1n}$$\n",
    "\n",
    "The **second principal component** is another linear combination of the $p$ features that accounts for the maximum of the _remaining_ variance after the first. Another condition is that the second PC must be **orthogonal (perpendicular)** to the first.\n",
    "\n",
    "$$ z_2 = c_{21}x_1 + c_{22}x_2 + c_{23}x_3 + ... c_{2n}$$\n",
    "\n",
    "The **third principal component** maximizes the remaining variance while being orthogonal (read: _independent_) to the first two, and so on.\n",
    "\n",
    "\n",
    "$$ z_3 = c_{31}x_1 + c_{32}x_2 + c_{33}x_3 + ... c_{3n}$$\n",
    "$$...$$\n",
    "$$ z_i = c_{i1}x_1 + c_{i2}x_2 + c_{i3}x_3 + ... c_{in}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MachineLearning](assets/PCA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands on visuals are a great way to learn. Here is an interactive website to help you understand how [PCA works](http://setosa.io/ev/principal-component-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"motivation\"></a>\n",
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<b>Dimensionality reduction</b> <i>intelligently</i> decreases the number of random variables that will be considered for analysis -- leaving just the most important ones.\n",
    "\n",
    "- Dimensionality reduction is not an end goal in itself but is instead a tool to form a data set with optimized features for further visualization and/or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> <b>Recall</b> that <i>collinear</i> variables are variables that can be linearly predicted from other variables with a substantial degree of accuracy.\n",
    "> - Eliminating <b>redundant features</b> such as these leaves us with a smaller dataset, with minimal information lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How can we detect collinearity in our dataset?\n",
    "To get a quick summary of our data, we can calculate a <b><i>covariance matrix</i></b> — an unstandardized correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\large\n",
    "\\begin{align}\n",
    "Var(X) & = \\begin{bmatrix}Var(X_1) & Cov(X_1,X_2)\\\\Cov(X_2,X_1) & Var(X_2)\\end{bmatrix} \\\\\n",
    "& = \\begin{bmatrix}2&1\\\\1&4\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "+ The diagonal elements in a covariance matrix show us the variance of each of our features.\n",
    "+ The off-diagonal elements show the covariance, i.e. the amount of collinearity (redundancy) between our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### What would an “ideal” covariance matrix look like?\n",
    "\n",
    "---\n",
    "\n",
    "An ideal covariance matrix for data would have <b>large numbers (variances) along the diagonal</b>, which would indicate a large amount of signal in the data. \n",
    "- It would also have <b>small values (or zeros) in the off-diagonal elements</b>, as these values indicate redundancy across our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What if we DO have redundant features?\n",
    "\n",
    "----\n",
    "\n",
    "What can we do to try to remove any redundancies *and* preserve the signal?\n",
    "\n",
    "- Surprise! We can use <i><b>PCA</b></i>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"what\"></a>\n",
    "## What is Principal Component Analysis (PCA)?\n",
    "\n",
    "---\n",
    "\n",
    "We've seen previously how we can use tools like <i>random forests</i> to accomplish \"<b>feature <i>selection</i></b>\". \n",
    "- Here, we'll focus on a form of dimensionality reduction known as <i>principal component analysis</i> that accomplishes \"<b>feature <i>extraction</i></b>\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whereas <b>feature <i>selection</i></b> attempts to reveal the subsets of the original data that are most relevant, <b>feature <i>extraction</i></b> combines features of a potentially correlated dataset resulting in a new <i>non-correlated</i> dataset with <i>fewer features</i>.\n",
    "\n",
    "PCA is currently among the most popular dimensionality reduction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### TL;DR:\n",
    "<i>Dimensionality reduction</i> is the process of combining or collapsing the existing features (columns in $X$) into fewer features. \n",
    "\n",
    "When we perform dimensionality reduction, we hope to:\n",
    "\n",
    "- Retain the signal in the original data.\n",
    "- Reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "PCA finds the linear combinations of the current predictor variables that create new variables, referred to as the \"principal components\". \n",
    "\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/1024px-GaussianScatterPCA.svg.png' alt='pca' width='350'></center>\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"//commons.wikimedia.org/wiki/User:Nicoguaro\" title=\"User:Nicoguaro\">Source</a></div>\n",
    "\n",
    "- The principle components are ranked in terms of how well they explain the variance in your original predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A more natural way of thinking about PCA is that <b>it transforms the coordinate system so that the axes become the most concise, informative descriptors of our data as a whole.</i>\n",
    "\n",
    "> The old axes are the original variables (columns).  \n",
    "> The new axes are the principal components from PCA.\n",
    "\n",
    "<center><img src='assets/pca_diagram.png' alt='pca diagram' width='1000'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Additional resources\n",
    "- [A great paper on PCA](http://arxiv.org/pdf/1404.1100.pdf)  \n",
    "- [Interactive PCA plot](https://setosa.io/ev/principal-component-analysis/)  \n",
    "- [An nice example of performing PCA](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Would We Want to Perform PCA?\n",
    "\n",
    "- We can <b><i>reduce the number of dimensions</i></b> (remove less important components) while retaining <i>most</i> of our signal (and removing <i>mostly</i> noise).\n",
    "- Because we’re assuming our variables are inter-related (at least in the sense that, together, they explain a dependent variable), the information of interest should exist along directions with the largest variance.\n",
    "    - As a result, the directions of largest variance should have the highest signal-to-noise ratio.\n",
    "- Correlated predictor variables (also referred to as redundancy of information) are combined into new, independent variables. \n",
    "    - Our predictors from PCA are <i>guaranteed to be independent</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But first, a few examples...\n",
    "\n",
    "> #### First, a descriptive example\n",
    "> <center><img src='https://upload.wikimedia.org/wikipedia/commons/9/90/PCA_fish.png' alt='pca on fish' width='500'></center>\n",
    "> <div style=\"text-align: right\"><a href=\"https://fr.wikipedia.org/wiki/Utilisateur:Lehalle\">Source</a></div>\n",
    ">\n",
    "> In this example, the two axes represented are the best way to represent 3-dimensional data in a two-dimensional plane, with the least possible loss of the original variance.\n",
    ">\n",
    "> Let's look at a similar example in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:49.557927Z",
     "start_time": "2022-02-08T10:50:49.387645Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# create data\n",
    "n = 50\n",
    "xs = np.random.normal(0, 5, n)\n",
    "ys = np.random.normal(0, 20, n)\n",
    "zs = np.random.normal(0, 5, n)\n",
    "\n",
    "# create plot\n",
    "ax.scatter(xs, ys, zs)\n",
    "ax.set_xlabel('X', fontsize=14)\n",
    "ax.set_ylabel('Y', fontsize=14)\n",
    "ax.set_zlabel('Z', fontsize=14)\n",
    "ax.set_xlim(-40, 40)\n",
    "ax.set_ylim(-40, 40)\n",
    "ax.set_zlim(-40, 40)\n",
    "\n",
    "###################################\n",
    "angle = 0 # try changing to zero #\n",
    "###################################\n",
    "\n",
    "# rotate the axes and show\n",
    "ax.view_init(30, angle)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A more applied example\n",
    "\n",
    "<center><img src='http://www.scholarpedia.org/w/images/thumb/a/a6/QQ_Fig1.jpg/400px-QQ_Fig1.jpg' alt='pca' width='500'></center>\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"http://www.scholarpedia.org/article/Spike_sorting\">Source</a></div>\n",
    "\n",
    "<i><b>Spike sorting</b></i> is the grouping of spikes into clusters based on the similarity of their shapes. \n",
    "- Since each neuron tends to exhibit spikes of a particular shape, the resulting clusters correspond to the activity of different neurons. \n",
    "- The result of spike sorting is the determination of which spike corresponds to which of these neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img style=\"float: right\" src='./assets/spike_sorting.jpg' alt='spike sorting' width='400'/>\n",
    "\n",
    "> #### How do we sort the spikes? \n",
    "> \n",
    "> One common method is to extract features of the spike shapes. \n",
    "> - But, the <i>major challenge</i> is how to select which are the best features!\n",
    "> - One (very popular) method for choosing features automatically is with PCA. \n",
    "> - The idea behind PCA is to find an ordered set of vectors that capture the directions containing the <i>greatest variation</i> in the data. \n",
    "> - The input data are the original spikes, and the output data are those same spikes, but with fewer dimensions.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b>Our final example</b> of how PCA can used is <i>reducing the statistical complexity of face images</i> (i.e. calculating <i><b>'eigen-faces'</b></i>).\n",
    "- PCA is performed on a large set of images depicting different human faces. \n",
    "- Eigenfaces can be considered a set of \"standardized face ingredients\", derived from statistical analysis of many pictures of faces.\n",
    "- See [here](https://en.wikipedia.org/wiki/Eigenface) for more info.\n",
    "\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Eigenfaces.png/220px-Eigenfaces.png' alt='eigenfaces' width='300'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Process of PCA \n",
    "\n",
    "---\n",
    "\n",
    "Say we have a matrix, $X$, of predictor variables. \n",
    "- PCA gives us the ability to transform our $X$ matrix into a new matrix, $Z$.\n",
    "\n",
    "<br></br>\n",
    "<center><img src='./assets/transformed_xyz.png' alt='3D transform' width='850'></center>\n",
    "<div style=\"text-align: right\"><a href=\"http://phdthesis-bioinformatics-maxplanckinstitute-molecularplantphys.matthias-scholz.de/\">Source</a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ First, we’ll derive a <b>weighting matrix</b>, $W$, from the correlational/covariance structure of $X$. \n",
    "    - This allows us to perform the transformation.\n",
    "\n",
    "+ Each successive dimension (column) in $Z$ will be ranked and ordered according to the variance in its values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Assumptions\n",
    "\n",
    "1. **Linearity:** The data do not hold nonlinear relationships.\n",
    "    - e.g.: 'hours worked' vs. '$ paid' when including overtime.\n",
    "    \n",
    "    \n",
    "2. **Large variances define importance:** The dimensions are constructed to maximize the remaining variance. \n",
    "    - We assume any orthogonal variance is due to noise rather than signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> <b>Yes!</b> #2 does mean it is <i>necessary to normalize data before performing PCA</i>. \n",
    "> - PCA calculates a new projection of your data set, and the new axis are based on the standard deviation of your variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting principal components (columns of $Z$) will be uncorrelated. \n",
    "- This also makes PCA a useful pre-processing step for algorithms requiring uncorrelated input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> <b>Yes!</b> This does mean that the <i>maximum number of principal components is the number of features</i>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"eigenpairs\"></a>\n",
    "### Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Eigenvectors\n",
    "\n",
    "<center><img src='./assets/eigenvectors_orthogonal.png' alt='orthogonal vectors' width='700'></center>\n",
    "\n",
    "An eigenvector specifies a direction through the original coordinate space.\n",
    "- The eigenvector with the highest corresponding eigenvalue is the first principal component (PC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='./assets/transformed_xy.png' alt='transformed vectors' width='700'></center>\n",
    "\n",
    "> In some sense, the first PC is just a line fitted to the data.\n",
    "> - It is the <i>linear surface</i> (hyperplane) that is <b>closest</b> to the $n$ observations.\n",
    "> - Such a line will likely provide a good summary of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='./assets/eigenvalue.png' alt='eigen values' width='700'></center>\n",
    "\n",
    "#### Eigenvalues\n",
    "\n",
    "Eigenvalues indicate the amount of variance in the direction of its corresponding eigenvector. \n",
    "- The larger the eigenvalue, the more variance (information) in our data its corresponding eigenvector explains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> #### Every eigenvector has a corresponding eigenvalue.\n",
    "> These are sometimes called <b>eigenpairs</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='./assets/pca_coord_trans.png' alt='pca transformations' width='1200'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"pcs\"></a>\n",
    "### Principal Components\n",
    "\n",
    "#### <i>Principal components</i> are the vectors that define the new coordinate system for your data.\n",
    "- Transforming your original data columns onto the principal component (PC) axes constructs new variables that explain as much variance as possible and are independent (uncorrelated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creating these variables is a well-defined mathematical process. \n",
    "- In essence, <b>each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [Explained Visually](http://setosa.io/ev/) has a nice [interactive visualization for PCA](http://setosa.io/ev/principal-component-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src='./assets/setosa_pc1.png' alt='setosa transformations' width='1000'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><b>Optional intermission</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"manual\"></a>\n",
    "## Manual PCA Code Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**PCA Steps**\n",
    "\n",
    "1. **Standardize data:** Centering is required. Full normalization is nice for future visuals.\n",
    "2. **Calculate eigenvectors and eigenvalues:** Do this from the correlation or covariance matrix.\n",
    "3. **Sort eigenvalues:** Choose eigenvectors that correspond to the largest eigenvalues. The number you choose is up to you, but we will take two for the sake of visualization here.\n",
    "4. **Construct the projection weighting matrix, $W$:** Do this from the eigenvectors.\n",
    "5. **Transform the original data set, $X$, with $W$:** Obtain the new two-dimensional transformed matrix, $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Data**\n",
    "\n",
    "We're going to be using a simple 75-row, four-column data set with generic demographic information. It contains:\n",
    "\n",
    "- `Age` (limited to 20–65)\n",
    "- `Income`\n",
    "- `Health` (A rating on a scale from 1:100, where 100 is the best health)\n",
    "- `Stress` (A rating on a scale from 1:100, where 100 is the most stressed)\n",
    "\n",
    "All of the variables are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:53.775467Z",
     "start_time": "2022-02-08T10:50:53.768118Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "demo_data = pd.read_csv('./datasets/simple_demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"basic-eda\"></a>\n",
    "### 1) Basic EDA.\n",
    "\n",
    "Create a Seaborn regplot for each:\n",
    "\n",
    "1. `Age` versus `income`.\n",
    "2. `Age` versus `health`.\n",
    "3. `Age` versus `stress`.\n",
    "\n",
    "Also make a pair plot of the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:54.786413Z",
     "start_time": "2022-02-08T10:50:54.631682Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax = sns.regplot('age', 'income', data=demo_data, fit_reg=False, scatter_kws={'s':50}, ax=ax)\n",
    "\n",
    "ax.set_ylabel('income', fontsize=15)\n",
    "ax.set_xlabel('age', fontsize=15)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.set_title('age vs income\\n', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:55.553560Z",
     "start_time": "2022-02-08T10:50:55.391051Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax = sns.regplot('age', 'health', data=demo_data, fit_reg=False, scatter_kws={'s':50}, ax=ax)\n",
    "\n",
    "ax.set_ylabel('health', fontsize=15)\n",
    "ax.set_xlabel('age', fontsize=15)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.set_title('age vs health\\n', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:56.155254Z",
     "start_time": "2022-02-08T10:50:55.933982Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax = sns.regplot('age', 'stress', data=demo_data, fit_reg=False, scatter_kws={'s':50}, ax=ax)\n",
    "\n",
    "ax.set_ylabel('stress', fontsize=15)\n",
    "ax.set_xlabel('age', fontsize=15)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.set_title('age vs stress\\n', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:58.794809Z",
     "start_time": "2022-02-08T10:50:56.582571Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# alternative, showing all possible pairs\n",
    "sns.pairplot(demo_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"subset\"></a>\n",
    "### 2) Subset and normalize.\n",
    "\n",
    "Subset the data to only include:\n",
    "\n",
    "- `Income`  \n",
    "- `Health`  \n",
    "- `Stress`\n",
    "\n",
    "We'll be comparing the principal components to `age` specifically, so we're leaving it out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:58.813739Z",
     "start_time": "2022-02-08T10:50:58.797466Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# subset data\n",
    "data_noage = demo_data[['health','income','stress']]\n",
    "\n",
    "# scale variables\n",
    "data_noage = (data_noage - data_noage.mean()) / data_noage.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"corr\"></a>\n",
    "### 3) Calculate the correlation matrix.\n",
    "\n",
    "We'll be using the correlation matrix to calculate the eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:58.841902Z",
     "start_time": "2022-02-08T10:50:58.822699Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# extract correlation matrix\n",
    "data_noage_corr = np.corrcoef(data_noage.values.T)\n",
    "data_noage.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"eigen\"></a>\n",
    "### 4) Calculate the eigenvalues and eigenvectors from the correlation matrix.\n",
    "\n",
    "NumPy has a convenient function to calculate this:\n",
    "\n",
    "`eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:50:59.355977Z",
     "start_time": "2022-02-08T10:50:59.349923Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(data_noage_corr)\n",
    "\n",
    "print(f'Eigenvalues: {eig_vals}\\n')\n",
    "print(f'Eigenvectors: {eig_vecs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"var\"></a>\n",
    "### 5) Calculate and plot the explained variance.\n",
    "\n",
    "A useful measure is the <b>explained variance</b>, which is calculated from the eigenvalues. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component:\n",
    "\n",
    "$$\\large ExpVar_i = \\bigg(\\frac{eigenvalue_i}{\\sum_j^n{eigenvalue_j}}\\bigg) * 100$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:00.283197Z",
     "start_time": "2022-02-08T10:51:00.277614Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "print(f'Cumulative Explained Variance: {cum_var_exp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:00.905859Z",
     "start_time": "2022-02-08T10:51:00.896489Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# let's define some functions to plot out the explained var. for each component\n",
    "\n",
    "component_number = [1,2,3]\n",
    "\n",
    "def scree_plot(ax):\n",
    "    # scree plot\n",
    "    ax.plot(component_number, var_exp, lw=7)\n",
    "    ax.axhline(y=0, linewidth=3, color='grey', ls='dashed')\n",
    "    ax.set_xlim([1,3])\n",
    "    ax.set_ylim([-5,105])\n",
    "    ax.set_ylabel('% variance explained', fontsize=16)\n",
    "    ax.set_xlabel('component', fontsize=16)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_xticks(component_number, minor=False)\n",
    "    ax.set_title('scree plot', fontsize=20)\n",
    "\n",
    "def cum_scree_plot(ax):\n",
    "    # cumulative scree plot\n",
    "    ax.plot(component_number, cum_var_exp, lw=7)\n",
    "    ax.axhline(y=100, linewidth=3, color='grey', ls='dashed')\n",
    "    ax.set_xlim([1,3])\n",
    "    ax.set_ylim([-5,105]) \n",
    "    ax.set_ylabel('cumulative % variance explained', fontsize=16)\n",
    "    ax.set_xlabel('component', fontsize=16)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_xticks(component_number, minor=False)\n",
    "    ax.set_title('cumulative scree plot', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:01.670278Z",
     "start_time": "2022-02-08T10:51:01.451729Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "scree_plot(ax)\n",
    "ax = plt.subplot(122)\n",
    "cum_scree_plot(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"projection\"></a>\n",
    "### 6) Construct the projection matrix, $W$.\n",
    "\n",
    "This is simply a matrix of our top $n$ (two, in this case) eigenvectors.\n",
    "\n",
    "The eigenvectors are concatenated as columns.\n",
    "\n",
    "1) Start by ordering the eigenvectors from largest to smallest by their corresponding eigenvalues.\n",
    "- Concatenate the eigenvectors together. `np.hstack()` is useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:02.393830Z",
     "start_time": "2022-02-08T10:51:02.386514Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sort eigenvectors by corresponding eigenvalues (largest to smallest)\n",
    "value_vector_pairs = [[eig_vals[i], eig_vecs[:,i]] for i in range(len(eig_vals))]\n",
    "# value_vector_pairs = list(zip(eig_vals, eig_vecs.T)) # also works\n",
    "value_vector_pairs.sort(reverse=True)\n",
    "value_vector_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:02.923305Z",
     "start_time": "2022-02-08T10:51:02.917612Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weight_2d_projection = np.hstack((value_vector_pairs[0][1].reshape(-1,1),\n",
    "                                  value_vector_pairs[1][1].reshape(-1,1)))\n",
    "\n",
    "print(f'Weight data 2D PCA projection matrix:\\n{weight_2d_projection}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"transformed\"></a>\n",
    "### 7) Construct the transformed two-dimensional matrix, $Z$.\n",
    "\n",
    "To do this, we take the <i>matrix product\\*</i> of our three-dimensional demographic matrix, $X$, with the projection matrix, $W$.\n",
    "\n",
    "<div style=\"text-align: right\"><i>*One of these again!?</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:03.805678Z",
     "start_time": "2022-02-08T10:51:03.796107Z"
    }
   },
   "outputs": [],
   "source": [
    "# recall our input\n",
    "data_noage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:04.252999Z",
     "start_time": "2022-02-08T10:51:04.242978Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# mat mult X with W to get Z\n",
    "Z = data_noage.dot(weight_2d_projection)\n",
    "Z.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"plot-components\"></a>\n",
    "### 8) Plot Principal Component 1 (PC1) vs. Principal Component 2 (PC2).\n",
    "\n",
    "Principal Component 1 is the first column in $Z$ and Principal Component 2 is the second.\n",
    "- Notice how they are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:05.407844Z",
     "start_time": "2022-02-08T10:51:05.271460Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "ax = plt.gca()\n",
    "ax = sns.regplot(Z.iloc[:,0], Z.iloc[:,1], fit_reg=False, scatter_kws={'s':50}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('principal component 1', fontsize=15)\n",
    "ax.set_ylabel('principal component 2', fontsize=15)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.set_title('PC1 vs PC2', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 9) Plot age vs. PC1 with `regplot`\n",
    "\n",
    "Notice how tight the relationship is. Principal Component 1 took the shared variance out of income, health, and stress, which are directly related to increasing age. \n",
    "\n",
    "This principal component, or more specifically the column-weighting matrix $W$, is essentially <b>capturing the latent age variance embedded in these variables.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:06.334232Z",
     "start_time": "2022-02-08T10:51:06.067475Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "ax = plt.gca()\n",
    "ax = sns.regplot(Z.iloc[:,0], demo_data.age.values,\n",
    "                 fit_reg=True, scatter_kws={'s': 50}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('principal component 1', fontsize=15)\n",
    "ax.set_ylabel('age', fontsize=15)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.set_title('PC1 vs age', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 10) Concatenate PC1 and PC2 to the full demographic (four-dimensional) data set, then melt it with PC1, PC2, and the index variables.\n",
    "\n",
    "1. Renormalize so that all four variables are on the same scale.\n",
    "2. Remember the Pandas melt code:\n",
    "\n",
    "```python\n",
    "melted_df = pd.melt(df, id_vars=['PC1','PC2'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:06.987240Z",
     "start_time": "2022-02-08T10:51:06.975387Z"
    }
   },
   "outputs": [],
   "source": [
    "demo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:07.448413Z",
     "start_time": "2022-02-08T10:51:07.428993Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# copy data\n",
    "demo_pcs = demo_data.copy()\n",
    "\n",
    "# scale data\n",
    "demo_pcs = (demo_data - demo_data.mean()) / demo_data.std()\n",
    "\n",
    "# add transformed data \n",
    "demo_pcs['PC1'] = Z.iloc[:,0]\n",
    "demo_pcs['PC2'] = Z.iloc[:,1]\n",
    "\n",
    "# convert from wide to long format\n",
    "demo_pcs = pd.melt(demo_pcs, id_vars=['PC1','PC2'])\n",
    "\n",
    "demo_pcs.sample(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 11) Use `lmplot` to view PC1 versus all four variables.\n",
    "\n",
    "Make the `col` keyword argument and the `hue` keyword argument \"variable,” assuming that's what you called them in the melt command (those are the defaults).\n",
    "\n",
    "Make `col_wrap = 2` and `size = 7`, or something similar, to make it visually appealing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:09.533026Z",
     "start_time": "2022-02-08T10:51:08.452225Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pc1 = sns.lmplot(x=\"PC1\", y=\"value\", col=\"variable\", hue=\"variable\", \n",
    "                 data=demo_pcs, col_wrap=2, scatter_kws={'s': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 12) Use `lmplot` to do the same for PC2.\n",
    "\n",
    "Notice how PC2 captures the variance of income, which was not captured well by PC1. \n",
    "- This makes sense, as the variance each principal component captures has to be orthogonal to the other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:10.712101Z",
     "start_time": "2022-02-08T10:51:09.536999Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pc2 = sns.lmplot(x=\"PC2\", y=\"value\", col=\"variable\", hue=\"variable\", \n",
    "                 data=demo_pcs, col_wrap=2, scatter_kws={'s':50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"auto\"></a>\n",
    "## PCA in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:10.724399Z",
     "start_time": "2022-02-08T10:51:10.714509Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA()\n",
    "X_r = pca.fit_transform(data_noage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:10.734568Z",
     "start_time": "2022-02-08T10:51:10.727493Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ratios = pca.explained_variance_ratio_\n",
    "print(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:10.799013Z",
     "start_time": "2022-02-08T10:51:10.794408Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:11.105438Z",
     "start_time": "2022-02-08T10:51:11.098158Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# this is only two components, column-by-column of the above\n",
    "weight_2d_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Real Data\n",
    "\n",
    "Let's work with the cancer data set again since it had so many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:11.816653Z",
     "start_time": "2022-02-08T10:51:11.796502Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cancer = load_breast_cancer()\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:51:12.069025Z",
     "start_time": "2022-02-08T10:51:12.063839Z"
    }
   },
   "outputs": [],
   "source": [
    "# If it's been awhile since you used a toy dataset - check out the description\n",
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:18.438242Z",
     "start_time": "2022-02-08T10:58:18.326111Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "#(['DESCR', 'data', 'feature_names', 'target_names', 'target'])\n",
    "ydf = pd.DataFrame(cancer['target'],columns=['malignant'])\n",
    "\n",
    "#Combined the dataframes\n",
    "df=pd.concat([X,ydf],axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Visualization\n",
    "\n",
    "As we've noticed before it is difficult to visualize high dimensional data, we can use PCA to find the first two principal components, and visualize the data in this new, two-dimensional space, with a single scatter-plot. Before we do this though, we'll need to scale our data so that each feature has a single unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:20.419370Z",
     "start_time": "2022-02-08T10:58:20.402910Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "scaled_data = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA with Scikit Learn uses a very similar process to other preprocessing functions that come with SciKit Learn. We instantiate a PCA object, find the principal components using the fit method, then apply the rotation and dimensionality reduction by calling transform().\n",
    "\n",
    "We can also specify how many components we want to keep when creating the PCA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:20.995044Z",
     "start_time": "2022-02-08T10:58:20.974110Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "#Now we can transform this data to its first 2 principal components\n",
    "\n",
    "x_pca = pca.transform(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:21.270628Z",
     "start_time": "2022-02-08T10:58:21.233336Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_import=pd.DataFrame(pca.components_,columns=cancer['feature_names'],index = ['PC-1','PC-2'])\n",
    "feat_import.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:22.041445Z",
     "start_time": "2022-02-08T10:58:21.715990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now that we've reduced our model to two dimensions - lets look at it\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='plasma')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly by using these two components we can easily separate these two classes.\n",
    "\n",
    "## Interpreting the components \n",
    "\n",
    "Unfortunately, with this great power of dimensionality reduction, comes the cost of being able to easily understand what these components represent.\n",
    "\n",
    "The components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:22.397743Z",
     "start_time": "2022-02-08T10:58:22.386592Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:22.717614Z",
     "start_time": "2022-02-08T10:58:22.709935Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(list(zip(pca.components_[0],df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this numpy matrix array, each row represents a principal component, and each column relates back to the original features. Fir example, here is the first component:\n",
    "\n",
    "$$z_1 = 0.21890244x_1 + 0.10372458x_2 + 0.22753729x_3 + 0.22099499x_4 + 0.14258969x_5 + ... + 0.13178394x_{30}$$\n",
    "\n",
    "we can visualize this relationship with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:23.883091Z",
     "start_time": "2022-02-08T10:58:23.522389Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(pca.components_,columns=cancer['feature_names'])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap shows the how each variable contributes to each of our two principle components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the right number of n_components?\n",
    "\n",
    "Finding the right number with PCA can be accomplished by applying it to a model and seeing where it best maximizing it's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:24.462243Z",
     "start_time": "2022-02-08T10:58:24.435395Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# How does it look with the whole feature set?\n",
    "\n",
    "lg=LogisticRegression()\n",
    "lg.fit(scaled_data,ydf)\n",
    "lg.score(scaled_data,ydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:24.714726Z",
     "start_time": "2022-02-08T10:58:24.702673Z"
    }
   },
   "outputs": [],
   "source": [
    "# How about reducing it to our two pca features?\n",
    "lg=LogisticRegression()\n",
    "lg.fit(x_pca,ydf)\n",
    "lg.score(x_pca,ydf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this tell us?\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "## Introducing Pipeline\n",
    "\n",
    "To find the right value of n_components we could cycle through this a few times. However - to save us some time lets introduce the sklearn package - [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "Pipeline sequentially applies a list of transforms and passes them to a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "Included **Methods**\n",
    "\n",
    "| Method                     | Application   |\n",
    "|:---------------------------|------------:|\n",
    "|decision_function(X)        |\tApply transforms, and decision_function of the final estimator|\n",
    "|fit(X[, y])                 |\tFit the model|\n",
    "|fit_predict(X[, y])         |\tApplies fit_predict of last step in pipeline after transforms.|\n",
    "|fit_transform(X[, y])       |\tFit the model and transform with the final estimator|\n",
    "|get_params([deep])          |\tGet parameters for this estimator.|\n",
    "|predict(X)                  |\tApply transforms to the data, and predict with the final estimator|\n",
    "|predict_log_proba(X)        |\tApply transforms, and predict_log_proba of the final estimator|\n",
    "|predict_proba(X)            |\tApply transforms, and predict_proba of the final estimator|\n",
    "|score(X[, y, sample_weight])|\tApply transforms, and score with the final estimator|\n",
    "|set_params(**kwargs)        |\tSet the parameters of this estimator.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:25.447217Z",
     "start_time": "2022-02-08T10:58:25.440247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizing for variance of n_components could take awhile. \n",
    "# Let's make our coding easier by putting all these models into a pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('pc', PCA(n_components=2)),\n",
    "    ('lg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What will the Standard Scalar do to our data before we fit each model?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:26.082182Z",
     "start_time": "2022-02-08T10:58:26.058707Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X,ydf)\n",
    "\n",
    "pipe.score(X,ydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:26.369902Z",
     "start_time": "2022-02-08T10:58:26.357292Z"
    }
   },
   "outputs": [],
   "source": [
    "#What's our model look like?\n",
    "pipe.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use the train/test split procedure from the previous lesson to see how each of these 30 versions performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:27.457918Z",
     "start_time": "2022-02-08T10:58:26.914459Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,ydf, random_state=1)\n",
    "\n",
    "# This for loop goes through every number of possible components and tests the accuracy of a model fitted to that many components.\n",
    "\n",
    "acc_list = []\n",
    "k_range = range(1,X.shape[1] + 1)\n",
    "for k in k_range:\n",
    "    pipe.set_params(pc__n_components=k)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    acc = pipe.score(X_test, y_test)\n",
    "    acc_list.append(acc)\n",
    "    print(f\"k = {k}: Acc = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:27.720328Z",
     "start_time": "2022-02-08T10:58:27.461619Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(acc_list);\n",
    "plt.ylabel('Accuracy - higher is better')\n",
    "plt.xlabel('Number of Components Included');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which number of n_components would you choose? why?\n",
    "\n",
    "#### Hint: what were we trying to reduce with PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:28.799875Z",
     "start_time": "2022-02-08T10:58:28.793221Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can visualize the list\n",
    "k_list=dict(zip(k_range,acc_list))\n",
    "k_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:29.372743Z",
     "start_time": "2022-02-08T10:58:29.366473Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can also find the greatest variance explained (for the first time)\n",
    "\n",
    "print(sorted(k_list.items(), key=lambda x: (x[1]), reverse=True)[0])\n",
    "print(sorted(k_list.items(), key=lambda x: (x[1]), reverse=True)[0][0])\n",
    "best_k=sorted(k_list.items(), key=lambda x: (x[1]), reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:30.132851Z",
     "start_time": "2022-02-08T10:58:30.047248Z"
    }
   },
   "outputs": [],
   "source": [
    "y = ydf\n",
    "#What happens when we add that back into our pipe?\n",
    "pipe_best_k = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('pc', PCA(n_components=14)),\n",
    "    ('lg', LogisticRegression())\n",
    "])\n",
    "pipe_best_k.fit(X,y)\n",
    "pipe_best_k.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead of setting n_components\n",
    "\n",
    "Another approach to PCA is setting it against the amount of variance you want to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:32.641090Z",
     "start_time": "2022-02-08T10:58:32.446344Z"
    }
   },
   "outputs": [],
   "source": [
    "# The first approch is to see how many component explain variance. After the first 4... not so much\n",
    "pca_graph=PCA().fit(cancer.data)\n",
    "plt.plot(np.cumsum(pca_graph.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:33.824082Z",
     "start_time": "2022-02-08T10:58:33.813221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's set a model that'll explain 95% of variance\n",
    "pca=PCA(.95)\n",
    "pca.fit(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hmmm... the boston dataset wasn't a great example. Let's find one a bit more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:35.875294Z",
     "start_time": "2022-02-08T10:58:35.661800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look across other toy datasets - diabetes is more complex\n",
    "\n",
    "diabetes = load_diabetes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:36.255975Z",
     "start_time": "2022-02-08T10:58:36.099808Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_graph=PCA().fit(diabetes.data)\n",
    "plt.plot(np.cumsum(pca_graph.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:36.638875Z",
     "start_time": "2022-02-08T10:58:36.631119Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ok - Bigger increase\n",
    "pca=PCA(.94)\n",
    "pca.fit(diabetes.data)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"example\"></a>\n",
    "## Eigenfaces: Let's kick it up a notch with something a bit more complex\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html#Example:-Eigenfaces\">Source</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:45.154758Z",
     "start_time": "2022-02-08T10:58:37.682317Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Because this is a large dataset, we will use RandomizedPCA—it contains a randomized method to approximate the first N principal components much more quickly than the standard PCA estimator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000). We will take a look at the first 150 components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:45.484362Z",
     "start_time": "2022-02-08T10:58:45.158466Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=150, svd_solver='randomized')\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as \"eigenvectors,\" so these types of images are often called \"eigenfaces\"). As you can see in this figure, they are as creepy as they sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:46.300622Z",
     "start_time": "2022-02-08T10:58:45.490709Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let's take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:46.586370Z",
     "start_time": "2022-02-08T10:58:46.305617Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:46.968066Z",
     "start_time": "2022-02-08T10:58:46.590769Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the components and projected faces\n",
    "pca = PCA(n_components=150, svd_solver='randomized').fit(faces.data)\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:58:47.655000Z",
     "start_time": "2022-02-08T10:58:46.970756Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
    "    \n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('150-dim\\nreconstruction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features. This visualization makes clear why the PCA feature selection used in In-Depth: Support Vector Machines was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image. What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- PCA is an unsupervised method used to reduce the dimensionality of data.\n",
    "- It does so by taking a <i>linear</i> combination of features.\n",
    "- It is common to plot the transformed data in PC space.\n",
    "- PCA searches for the directions that have the largest variance in the data.\n",
    "- All principal components are orthogonal to each other.\n",
    "\n",
    "\n",
    "# Summary:\n",
    "\n",
    "### What does it do?\n",
    "* Creates *linearly independent* predictors\n",
    "* Allows you to only use the most valuable features\n",
    "\n",
    "### What are the components?\n",
    "* Values calculated from the raw observations:\n",
    "\n",
    "$$ z_1 = c_{11}x_1 + c_{12}x_2 + c_{13}x_3 + ... c_{1n}$$\n",
    "$$ z_2 = c_{21}x_1 + c_{22}x_2 + c_{23}x_3 + ... c_{2n}$$\n",
    "$$...$$\n",
    "$$ z_i = c_{i1}x_1 + c_{i2}x_2 + c_{i3}x_3 + ... c_{in}$$\n",
    "\n",
    "* $z_1$ is always the strongest predictor. \n",
    "* $z_2$ is always the strongest predictor that is completely independent from $z_1$.  \n",
    "* $z_3$ is always the strongest predictor that is completely independent from $z_1$ and $z_2$.  \n",
    "* We can keep this going until the number of compoenents equals the number of predictors\n",
    "\n",
    "### Why do we exclude components from our model?\n",
    "* If we included all components, we would be getting the exact same result as if we hadn't used PCA\n",
    "* By excluding components we can avoid overfitting the model, we are essentially ignoring the information that is least reliable.\n",
    "\n",
    "### Points to Remember\n",
    "* PCA is used to overcome features redundancy in a data set.\n",
    "* These features are low dimensional in nature.\n",
    "* These features a.k.a components are a resultant of normalized linear combination of original predictor variables.\n",
    "* These components aim to capture as much information as possible with high explained variance.\n",
    "* The first component has the highest variance followed by second, third and so on.\n",
    "* The components must be uncorrelated (remember orthogonal direction ? ). See above.\n",
    "* Normalizing data becomes extremely important when the predictors are measured in different units.\n",
    "* PCA works best on data set having 3 or higher dimensions. Because, with higher dimensions, it becomes increasingly difficult to make interpretations from the resultant cloud of data.\n",
    "* PCA is applied on a data set with numeric variables.\n",
    "* PCA is a tool which helps to produce better visualizations of high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"more-reading\"></a>\n",
    "### More Useful Links, Reading, and References for Images\n",
    "\n",
    "- [PCA 4 Dummies](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "- [Stack Overflow: Making Sense of PCA](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n",
    "- [PCA and Spectral Theorem](http://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit)\n",
    "- [PCA in Three Steps: Eigendecomposition and SVD](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)\n",
    "- [Tutorial on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "- [PCA Math and Examples](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Onwards!\n",
    "... to the [lab](./practice/intro_to_PCA-lab.ipynb)!\n",
    "\n",
    "<center><img src='https://www.publicdomainpictures.net/pictures/290000/velka/lab-doctor.jpg' alt='lab' width='400'></center>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
