{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction with PCA\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td><img src=\"http://www.acheronanalytics.com/uploads/9/8/6/3/98636884/editor/51764130_1.jpg?1491762379\" style=\"float: center; width: 200px\"></td>\n",
    "    <td>\n",
    "        <b>Principal components analysis (PCA) is one of the most popular methods available<br>for reducing the number of variables in a data set.</b><br><br>\n",
    "        <li>We typically describe PCA as an unsupervised learning tool.</li><br>\n",
    "        <li>But, dimensionality reduction techniques are useful for supervised learning, too.</li><br>\n",
    "        <br><i>In this notebook,</i><br>we describe its use as a dimension reduction step for linear regression.\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we can use linear regression to model the relationship between our dependent variable and one (or more) independent variables (i.e. 'features').\n",
    "- Let's try using the principal components (the dimensions along which the data vary the most) as the features of our logistic regression and see how it affects our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the diabetes dataset\n",
    "data = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# define X\n",
    "feature_matrix = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "scaled_feature_matrix = pd.DataFrame(scaler.fit_transform(data.data), columns=data.feature_names)\n",
    "\n",
    "# define y\n",
    "labels = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize correlations in raw data using `PairGrid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(scaled_feature_matrix)\n",
    "g = g.map_lower(sns.regplot)\n",
    "g = g.map_upper(sns.kdeplot, cmap=\"Blues\", shade=True, shade_lowest=False)\n",
    "g = g.map_diag(plt.hist)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check linear regression scores before modifying data\n",
    "linreg = LinearRegression()\n",
    "orig_lr_scores = cross_val_score(linreg, scaled_feature_matrix, labels, scoring='neg_mean_squared_error', cv=25)\n",
    "\n",
    "print(np.sqrt(-(orig_lr_scores).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract principal components \n",
    "\n",
    "# if not specified: n_components = min(n_samples, n_features)\n",
    "# thus, in this case, n_components = 10, since n_features = 10\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(scaled_feature_matrix)\n",
    "pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the principal component weighting vectors (i.e. eigenvectors).\n",
    "- The principal components, or eigenvectors, can be thought of as weightings on the original variables to transform them into the new feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_names = [f'PC{i+1}' for i in range(len(pca.components_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_matrix.columns)\n",
    "for i, pc in enumerate(pc_names):\n",
    "    print(pc, 'weighting vector:', pca.components_[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the original data into the principal component space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat_pcs = pd.DataFrame(pca.transform(scaled_feature_matrix), columns=pc_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize correlations in PC's using [`PairGrid`](https://seaborn.pydata.org/generated/seaborn.PairGrid.html).\n",
    "- Confirm that correlations between variables have been eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(feat_mat_pcs)\n",
    "g = g.map_lower(sns.regplot)\n",
    "g = g.map_upper(sns.kdeplot, cmap=\"Blues\", shade=True, shade_lowest=False)\n",
    "g = g.map_diag(plt.hist)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, check linear regression scores for the reduced data\n",
    "linreg = LinearRegression()\n",
    "pc_lr_scores = cross_val_score(linreg, feat_mat_pcs, labels, scoring='neg_mean_squared_error', cv=25)\n",
    "\n",
    "print(pc_lr_scores)\n",
    "print(np.sqrt(-(pc_lr_scores).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we arrived at a model with very similar performance to the larger model, but with the number of features greatly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we wrap up --\n",
    "We should look at how we can use [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to 'merge' the two steps (PCA then LR) into a single object (see [example](https://scikit-learn.org/0.18/auto_examples/plot_digits_pipe.html) from docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('predict', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_scores = cross_val_score(pipe, scaled_feature_matrix, labels, scoring='neg_mean_squared_error', cv=25)\n",
    "print(pipe_scores)\n",
    "print(np.sqrt(-(pipe_scores).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In practice, we could then go on to use [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find the optimal number of components to use (if we were dealing with a larger data set). \n",
    "- But, we'll end there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
