{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# PCA Lab: Speed Dating\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice principal component analysis (PCA) using a data set from [Kaggle](https://www.kaggle.com/). \n",
    "- PCA is often used to simplify data, reduce noise, and find unmeasured latent variables, so let's take a look at an example of each and try to understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Explore how PCA relates to correlation.\n",
    "- Use PCA to perform dimensionality reduction.\n",
    "- Predict whether or not a speed dater likes reading based on the dater's other likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "The data set we're using for this lab is a subset of this [much more detailed speed dating data set](https://www.kaggle.com/annavictoria/speed-dating-experiment). \n",
    "- In particular, this contains no information on the actual speed dating itself (i.e., successes with or opinions of other individuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also contains no follow-up information where individuals are asked the same questions about themselves again. It only contains information about what an individual enjoys doing, their self-ratings on how desirable they are, and how they think others rate them based on desirability.\n",
    "\n",
    "The columns present in the data are outlined below:\n",
    "\n",
    "FieldName|Description\n",
    "---------|-----------\n",
    "    subject_id                   |   Unique individual identifier\n",
    "    wave                         |   Meetup ID\n",
    "    like_sports                  |   Enjoyment of participating in sports\n",
    "    like_tvsports                |   Enjoyment of watching sports on TV\n",
    "    like_exercise                |   Enjoyment of exercise\n",
    "    like_food                    |   Enjoyment of food\n",
    "    like_museums                 |   Enjoyment of museums\n",
    "    like_art                     |   Enjoyment of art\n",
    "    like_hiking                  |   Enjoyment of hiking\n",
    "    like_gaming                  |   Enjoyment of playing games\n",
    "    like_clubbing                |   Enjoyment of going clubbing/partying\n",
    "    like_reading                 |   Enjoyment of reading\n",
    "    like_tv                      |   Enjoyment of TV in general\n",
    "    like_theater                 |   Enjoyment of the theater (plays, musicals, etc.)\n",
    "    like_movies                  |   Enjoyment of movies\n",
    "    like_concerts                |   Enjoyment of concerts\n",
    "    like_music                   |   Enjoyment of music\n",
    "    like_shopping                |   Enjoyment of shopping\n",
    "    like_yoga                    |   Enjoyment of yoga\n",
    "    subjective_attractiveness    |   How attractive they rate themselves\n",
    "    subjective_sincerity         |   How sincere they rate themselves\n",
    "    subjective_intelligence      |   How intelligent they rate themselves\n",
    "    subjective_fun               |   How fun they rate themselves\n",
    "    subjective_ambition          |   How ambitious they rate themselves\n",
    "    objective_attractiveness     |   Perceived rating others would give them on how attractive they are\n",
    "    objective_sincerity          |   Perceived rating others would give them on how sincere they are\n",
    "    objective_intelligence       |   Perceived rating others would give them on how intelligent they are\n",
    "    objective_fun                |   Perceived rating others would give them on how fun they are\n",
    "    objective_ambition           |   Perceived rating others would give them on how ambitious they are\n",
    "    \n",
    "There are 551 subjects total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:09.272137Z",
     "start_time": "2020-08-04T22:51:03.869167Z"
    }
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2) Load and clean the speed dating data\n",
    "\n",
    "- First, remove columns with more than 200 missing values.\n",
    "- Then, remove rows with missing values.\n",
    "- Verify that no rows contain NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:09.315806Z",
     "start_time": "2020-08-04T22:51:09.275659Z"
    }
   },
   "outputs": [],
   "source": [
    "sd = pd.read_csv('../datasets/speed_dating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:09.353608Z",
     "start_time": "2020-08-04T22:51:09.320871Z"
    }
   },
   "outputs": [],
   "source": [
    "sd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:09.520771Z",
     "start_time": "2020-08-04T22:51:09.418472Z"
    }
   },
   "outputs": [],
   "source": [
    "sd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:09.688503Z",
     "start_time": "2020-08-04T22:51:09.550479Z"
    }
   },
   "outputs": [],
   "source": [
    "sd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:09.826553Z",
     "start_time": "2020-08-04T22:51:09.699034Z"
    }
   },
   "outputs": [],
   "source": [
    "sd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:09.894840Z",
     "start_time": "2020-08-04T22:51:09.831589Z"
    }
   },
   "outputs": [],
   "source": [
    "sd.drop(['objective_attractiveness','objective_sincerity',\n",
    "         'objective_intelligence','objective_fun','objective_ambition'],\n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:10.020569Z",
     "start_time": "2020-08-04T22:51:09.958553Z"
    }
   },
   "outputs": [],
   "source": [
    "sd.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:10.121863Z",
     "start_time": "2020-08-04T22:51:10.054976Z"
    }
   },
   "outputs": [],
   "source": [
    "sd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3) Example: Are the `subjective` columns correlated?\n",
    "\n",
    "Here, we'll understand how the `subjective` columns are correlated.\n",
    "\n",
    "- Find the z scores of each `subjective` column.\n",
    "- Visualize correlation using PairGrid.\n",
    "- Visualize correlation using a heat map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A) Find the z scores of each column. This allows the columns to more easily be directly compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:10.263558Z",
     "start_time": "2020-08-04T22:51:10.157364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subjective_cols = [col for col in sd.columns if col.startswith('subjective')]\n",
    "print(subjective_cols)\n",
    "subjective = sd[subjective_cols]\n",
    "subjective = (subjective - subjective.mean()) / subjective.std() # transform to z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B) Use a PairGrid to visualize correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:51:54.899548Z",
     "start_time": "2020-08-04T22:51:10.275883Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(subjective)\n",
    "g = g.map_lower(sns.regplot)    # regression plots in lower triangle\n",
    "g = g.map_upper(sns.kdeplot, cmap=\"Blues\", shade=True, shade_lowest=False)  # KDE plots in upper triangle\n",
    "g = g.map_diag(plt.hist)        # histograms along diagonal\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C) Use a heat map to visualize correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:52:33.073626Z",
     "start_time": "2020-08-04T22:52:33.052875Z"
    }
   },
   "outputs": [],
   "source": [
    "subj_corr = subjective.corr()      # correlation DataFrame â€” very useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:52:34.508783Z",
     "start_time": "2020-08-04T22:52:33.984093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle (taken from Seaborn example gallery)\n",
    "mask = np.zeros_like(subj_corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True         # triu: TRIangle upper\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "\n",
    "# Plot the heat map with Seaborn\n",
    "# Assign the Matplotlib axis the function returns. This will let us resize the labels\n",
    "ax = sns.heatmap(subj_corr, mask=mask)\n",
    "ax.set_ylim(subj_corr.shape[0],0)\n",
    "\n",
    "# Resize the labels\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n",
    "\n",
    "# If you put plt.show() at the bottom, it prevents useless printouts from Matplotlib\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:52:35.267860Z",
     "start_time": "2020-08-04T22:52:35.229093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Understand how this visualization can be \"seen\" just by looking at the correlation scores\n",
    "\n",
    "subj_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Important: Did you ensure the results make sense intuitively?** If not, look at the results again. You should **always** interpret your results and ensure they make sense based on what you expected. If they donâ€™t, investigate why â€” often your analysis or data are wrong.\n",
    "\n",
    "> For example, the results show that believing you are attractive and fun are correlated. Would you expect that believing you are intellectual and fun to have a higher or lower correlation? What do the results say?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 4) Visualize some preference columns.\n",
    "\n",
    "Next, weâ€™ll explore how some preference ratings are correlated. You saw an example â€” now try it on the `preference_cols` below.\n",
    "\n",
    "- Find the z scores of each column in `preference_cols` ([example](https://stackoverflow.com/a/41713622/6293191)).\n",
    "- Visualize correlation using PairGrid.\n",
    "- Visualize correlation using a heat map.\n",
    "- Do these results make sense intuitively? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:52:36.651256Z",
     "start_time": "2020-08-04T22:52:36.633153Z"
    }
   },
   "outputs": [],
   "source": [
    "preference_cols = ['like_tvsports', 'like_sports', 'like_museums', 'like_theater', 'like_shopping']\n",
    "sd_like = sd[preference_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.A) Find the z scores of each column in `preference_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:52:37.788550Z",
     "start_time": "2020-08-04T22:52:37.771440Z"
    }
   },
   "outputs": [],
   "source": [
    "sd_like = (sd_like - sd_like.mean()) / sd_like.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.B) Visualize correlation using PairGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:53:23.501499Z",
     "start_time": "2020-08-04T22:52:39.571255Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(sd_like)\n",
    "g = g.map_lower(sns.regplot)\n",
    "g = g.map_upper(sns.kdeplot, cmap=\"Blues\", shade=True, shade_lowest=False)\n",
    "g = g.map_diag(plt.hist)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.C) Visualize correlation using a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_corr = sd_like.corr()\n",
    "\n",
    "mask = np.zeros_like(pref_corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "\n",
    "ax = sns.heatmap(pref_corr, mask=mask)\n",
    "ax.set_ylim(pref_corr.shape[0],0)\n",
    "\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5) Example: Fit PCA on the `subjective` ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate PCA model (specify number of components)\n",
    "subjective_pca = PCA(n_components=5)\n",
    "\n",
    "# fit PCA model ('learn' the data)\n",
    "subjective_pca.fit(subjective.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 5.A) Look at principal component weighting vectors (eigenvectors).\n",
    "\n",
    "The principal components, or eigenvectors, can be thought of as weightings on the original variables to transform them into the new feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_components = subjective_pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subjective_cols, '\\n')\n",
    "print('-------------------------------------\\n')\n",
    "\n",
    "for i, pc in enumerate(['PC1','PC2','PC3','PC4','PC5']):\n",
    "    print(pc, 'weighting vector:', subj_components[i])\n",
    "    print( '-------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 5.B) Look at the eigenvalues and the explained variance ratio.\n",
    "\n",
    "The eigenvalues are ordered such that the first components have the largest eigenvalues. The values and their normalized equivalent in the explained variance ratio attribute tell you how much of the variance in the original data is encapsulated in the new component variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_exp_var_eigenvals = subjective_pca.explained_variance_\n",
    "subj_exp_var_pct = subjective_pca.explained_variance_ratio_\n",
    "\n",
    "print('eigenvalues:', subj_exp_var_eigenvals, '\\n')\n",
    "print('explained variance pct:', subj_exp_var_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 5.C) Transform the subjective data into the principal component space.\n",
    "\n",
    "The `transform()` function in the PCA will create your new component variable matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform values using weights of PCA object\n",
    "subj_to_pcs = subjective_pca.transform(subjective.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformed our five-dimensional data set into vectors along its five principal components (with zero loss).\n",
    "- Using these, we can now reduce the dimensionality of our data while minimizing loss.\n",
    "- For example, taking only the first three eigenvectors accounts for $0.431 + 0.178 + 0.147 = 75.6\\%$ of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This transforms our original five-dimensional data into three-dimensional data\n",
    "# The first row is the first person's subjective.values transformed\n",
    "\n",
    "subj_to_pcs[:,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 5.D) PCA-transformed features are not correlated.\n",
    "\n",
    "- Keep in mind that each column in the transformed data is no longer correlated.\n",
    "- Compare this to the exploration above, where many columns were correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(subj_to_pcs, columns=['PC1','PC2','PC3','PC4','PC5']), kind='reg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6) Optional: How were the data transformed?\n",
    "\n",
    "To demonstrate how the new principal component matrix is created from the original variable columns and the eigenvector weighting matrix, we'll create the first component (PC1) manually.\n",
    "\n",
    "#### 6.A) Pull out the eigenvector for PC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_weights = subj_components[0]\n",
    "\n",
    "person1_original_ratings = subjective.iloc[0,:]\n",
    "person1_pcas = subj_to_pcs[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.B) Create a DataFrame showing the original values for the subjective variables for `person1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1_original_ratings = subjective.iloc[0,:]\n",
    "\n",
    "how_to_make_pc1 = pd.DataFrame({'person1_original': person1_original_ratings.values},\n",
    "                               index=subjective.columns)\n",
    "how_to_make_pc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.C) Add the eigenvector for PC1: the weights by which to multiply each original variable.\n",
    "\n",
    "Recall that each component is a linear combination of the original variables, multiplied by a \"weight\" defined in the eigenvector of that component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_to_make_pc1['weights_to_make_pc1'] = confidence_weights\n",
    "how_to_make_pc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.D) Multiply the original variable values by the eigenvector values.\n",
    "\n",
    "These are the \"pieces\" of PC1 that will be added together to create the new value for that person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_to_make_pc1['pieces_of_pc1_value'] = how_to_make_pc1.person1_original * how_to_make_pc1.weights_to_make_pc1\n",
    "how_to_make_pc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.E) Sum the original values multiplied by the eigenvector weights to get `person1`â€™s value for PC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sum of linear combinations of weights * original values for PC1:', np.sum(how_to_make_pc1.pieces_of_pc1_value))\n",
    "print('person 1s pca variables:', person1_pcas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7) Fit PCA on the preference data.\n",
    "\n",
    "Now that you've seen how it's done, try it yourself!\n",
    "\n",
    "- Find PCA eigenvalues and eigenvectors for the five `sd_like` columns.\n",
    "- Transform the original `sd_like` columns into the principal component space.\n",
    "- Verify that these columns are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_like.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.A) Find PCA eigenvalues and eigenvectors for the five `sd_like` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_pca = PCA(n_components=5)\n",
    "pref_pca.fit(sd_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comp = pref_pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pref_pca.explained_variance_ratio_)\n",
    "print('-------------------------------------\\n')\n",
    "\n",
    "print(sd_like.columns.values)\n",
    "print('-------------------------------------\\n')\n",
    "\n",
    "for i, pc in enumerate(['PC1','PC2','PC3','PC4','PC5']):\n",
    "    print(pc, 'weighting vector:', pref_comp[i])\n",
    "    print('-------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.B) Transform the original `sd_like` columns into the principal component space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_pcs = pref_pca.transform(sd_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_pcs[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.C) Verify that these columns are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(pref_pcs, columns=['PC1','PC2','PC3','PC4','PC5']), kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8) Use PCA for dimensionality reduction.\n",
    "\n",
    "Using linear regression, let's predict whether or not a user likes reading.\n",
    "\n",
    "**The key question:** Can we get the same prediction accuracy using only the first three principal components as features versus using all five original values as features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "reading = sd['like_reading'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.A) Linear regression cross-validated on original variables (`sd_like.values`).\n",
    "\n",
    "- What is the mean cross-validation score?\n",
    "- Keep in mind that linear regression uses error for score, so zero is the ideal score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "original_scores = cross_val_score(linreg, sd_like.values, reading, cv=10)\n",
    "print(sd_like.columns.values)\n",
    "print(original_scores)\n",
    "print(np.mean(original_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(sd_like.values, reading)\n",
    "for coef, var in zip(linreg.coef_, sd_like.columns):\n",
    "    print(var, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.B) Linear regression on the first principal component.\n",
    "\n",
    "- What is the mean cross-validation score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_pcs[:,0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_linreg = LinearRegression()\n",
    "pca_scores = cross_val_score(pca_linreg, pref_pcs[:,0:1], reading, cv=10)\n",
    "print(pca_scores)\n",
    "print(np.mean(pca_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.C) Linear regression on first three principal components.\n",
    "\n",
    "- What is the mean cross-validation score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_linreg = LinearRegression()\n",
    "pca_scores = cross_val_score(pca_linreg, pref_pcs[:,0:3], reading, cv=10)\n",
    "print(pca_scores)\n",
    "print(np.mean(pca_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Checkity-check yo'self**. The mean cross-validation score should be nearly the same for the first three principal components as it was on the original five-component data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
